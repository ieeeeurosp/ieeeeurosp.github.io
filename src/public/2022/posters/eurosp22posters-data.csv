ID,Title,Authors,Abstract,# Reviews,Status
4,Towards Cyber Resilience of Cyber-Physical Systems using Tiny Twins,"Fereidoun Moradi, Sara Abbaspour Asadollah, and Marjan Sirjani (Malardalen University)","We propose a method to detect attacks on sensor data and control commands in cyber-physical systems. We develop a monitor module that uses an abstract digital twin, Tiny Twin, to detect false sensor data and faulty control commands. The Tiny Twin is a state transition system that represents the observable behavior of the system. The monitor observes the sensor data and the control commands transmitted in the network, walks over the Tiny Twin and checks whether the  observed data and commands are consistent with the transitions in the Tiny Twin. The monitor produces an alarm when an attack is detected. The Tiny Twin is built automatically based on a timed actor code of the system. We demonstrate the method and evaluate it in detecting attacks using a temperature control system.",0,Accepted
7,A practical methodology for ML-Based EM Side Channel Disassemblers,"Cesar Arguello, Hunter Searle, Sara Rampazzi, and Kevin Butler (University of Florida)","Providing security guarantees for embedded devices with limited interface capabilities is an increasingly crucial task. Although these devices don’t have traditional interfaces, they still generate unintentional electromagnetic signals that correlate with the instructions being executed. By collecting these traces using our methodology and leveraging a random forest algorithm to develop a machine learning model, we built an EM side channel based instruction level disassembler. The disassembler was tested on an Arduino UNO board, yielding an accuracy of 88.69% instruction
recognition for traces from twelve instructions captured at a single location in the device; this is an improvement compared to the 75.6% (for twenty instructions) reported in previous similar work.",0,Accepted
9,The Beauty and the Beast (40 years of process algebra and cybersecurity),"Silvia De Francisci, Gabriele Costa, and Rocco De Nicola (IMT School for Advanced Studies Lucca)","Process algebras provide the mathematical foundation for several formal verification techniques, and they profoundly influenced many fields, from correct design to testing. Process algebras were greatly influential also for the security community. One of the main reasons for their success is their compact, yet expressive and flexible syntax, which allows to model the relevant aspects of computation while abstracting away the secondary ones. Although most authors acknowledge the importance of process algebras for the security community, it is not trivial to estimate how they
shaped the past and present researches.
The goal of this work is to provide a comprehensive outlook on some prominent works about process algebras and security. These include both the application of process algebras to security problems and process algebras inspired by security-related aspects of computation. To achieve this, we consider three fundamental fields of cybersecurity, i.e., secure development, threat modeling, and vulnerability assessment.",0,Accepted
11,Exploiting Timing Side-Channel Leaks in Web Applications that Tell on Themselves,"Vik Vanderlinden, Tom Van Goethem, Wouter Joosen, and Mathy Vanhoef (imec-DistriNet, KU Leuven)","The performance of remote timing attacks is highly dependent on the network connection that the attack is executed over, where jitter in both the up- and downstream direction can significantly deteriorate an attack's performance. Traditional timing attacks overcome this problem by obtaining a large number of measurements.

In this poster, we present a technique to remove the inaccuracies caused by downstream jitter in a remote timing attack, which we expect to reduce the number of measurements required to perform a successful timing attack. Our core idea is to exploit timestamps in HTTP responses, whose values are independent of the downstream jitter. To abuse these timestamps, the adversary synchronizes with the target web server's clock edge, after which the observed timestamps allow the adversary to infer secret information.

We present a method to synchronize with the server's clock and discuss how to compensate for the clock drift between the attacker and target machines. To evaluate the feasibility of our technique, we also investigate the occurrence of timestamps in HTTP responses for the top 10,000 sites according to the Tranco list.",0,Accepted
12,TESTABLE: Testability-driven security and privacy testing for Web Applications,Luca Compagna (SAP Security Research); Giancarlo Pellegrino (CISPA Helmholtz Center for Information Security); Davide Balzarotti (Eurecom); Martin Johns (Technical University Braunschweig); Ángel Cuevas (Universidad Carlos III de Madrid); Battista Biggio (Pluribus One); Leyla Bilge (Norton Lifelock); Fabian Yamaguchi (ShiftLeft); Matteo Meucci (IMQ Minded Security),"Modern web applications play a pivotal role in our digital society. Motivated by the many security vulnerabilities and data breaches routinely reported on those applications, we initiated the EU TESTABLE research project to address the main challenges of building and maintaining web applications secure and privacy-friendly. The ultimate goal is to lay the foundations for a new integration of security and privacy into the software development lifecycle (SDLC), by proposing the novel idea of combining two metrics to quantify the security and privacy risks of a program, i.e., the testability of the codebase and the indicators for vulnerable behaviors. Based on its novel concept of ""testability patterns"", we aim to empower the main SDLC actors (e.g., software and AI developers, managers, testers, and auditors) to reduce the security and privacy risks computed for their web applications by (i) building better security and privacy testing techniques for classical and AI-powered web applications, and (ii) removing or mitigating the impact of the testability patterns responsible for the high-risk levels. 

Promising results have been already achieved by applying our research proposal in the area of static analysis security testing (SAST) that led to a paper accepted at NDSS 2022. Hundreds of code instructions challenging for SAST tools have been identified, used to measure SAST tools effectiveness, and partially refactored to improve the ""SAST testability"" of open source applications.  More than 180 new vulnerabilities have been uncovered after refactoring and confirmed by open source applications' owners (including 38 vulnerabilities impacting popular Github projects with more than 1000 stars). 

We believe similar promising results can also be achieved in other areas such as dynamic analysis, privacy, and machine learning.",0,Accepted
13,Detecting Network Anomalies from Small Traffic Samples using Graph Neural Network,Aviv Yehezkel and Eyal Elyashiv (Cynamics),"Detecting anomalies in computer networks is a classic, long-term research problem.  
While almost every kind of model architecture has been proposed, previous works usually analyzed the entire network traffic. 
However, such analysis implies high memory and processing overhead, and is becoming less applicable for large networks. In this poster we present a work in progress which studies a previously under-researched setting where only a small fraction of the network traffic is given. 
Our approach pre-processes the samples and transforms the computer network into a graph neural network (GNN). It distills node features, edge features and graph representations to learn a vector embedding for each endpoint which characterizes its normal behaviors. Then, a link predictor model is used to estimate the likelihood of network communications and detect anomalies.",0,Accepted
14,How Attackers Determine the Ransom in Ransomware Attacks,"Tom Meurs, Marianne Junger, and Abhishta Abhishta (University of Twente)","Ransomware attacks have gained more attention over the past years [1]. Most ransomware attackers are financially motivated [6,7]. It is however still unknown how much financial gains criminals hope to make . This study aims to empirically study how much gains criminals hope to make by analysing the requested ransom. Based on the Rational Choice Perspective (RCP) [2], [3] we hypothesise in this study that requested ransom depends on how much effort criminals put in a ransomware attack. Furthermore, from Routine Activity Theory (RAT) [8] we hypothesize that victims with large revenue provide the opportunity for criminals to earn more money and therefore will demand higher ransom [5]. We investigated N=371 ransomware attacks registered by the Dutch Police between 2019 and 2021. Effort was measured by criminals performing data exfiltration, personalizing the requested ransom, collaboration with other criminals and targeting NAS devices. Contextual variables included yearly revenue of the victim, being insured against ransomware, having backups, sector, staff, season and year. Performing stepwise regression, we found a relationship between requested ransom and data exfiltration, personalizing the requested ransom, collaboration with other criminals, NAS and yearly revenue. Furthermore, we found a selection bias [4] of requested ransomware. We found no effect of year and season on requested ransom. Concluding, more effort and large revenue lead to higher requested ransom. The present study has provided clear support that RCP and RAT are applicable to ransomware attacks. These findings suggest several courses of action for policy makers and Law Enforcement.",0,Accepted
19,A boostershot for transferable physically realizable adversarial examples,"Willem Verheyen, Sander Joos, Tim Van hamme, Davy Preuveneers, and Wouter Joosen (imec-DistriNet, KU Leuven)","Adversarial perturbations are claimed to enlarge
the attack surface of machine learning models. However, as
the most prominent attack methodologies require unrealistically
strong adversaries, they are hardly used in attacks against real-
world systems. In this paper, we alleviate the constraints on the
threat model and attack a face recognition system with physically
realizable perturbations in a black-box scenario, provided a single
attack attempt. As such, we are forced to rely on more pragmatic,
but less effective, attack methods that leverage transferability –
adversarial perturbations successful on known models tend to
also work on unknown ones. We overcome the poor attack success
rate of transferability by using adversarially trained surrogate
models. The evaluation of our method demonstrates the
benefits of this approach for physically realizable adversarial
examples, with a significant increase in the transfer rates with a
factor of at least 1.5 up to 7 compared to the state-of-the-art.",0,Accepted
20,One of a Kind: Correlating Robustness to Adversarial Examples and Face Uniqueness,"Giuseppe Garofalo, Tim Van hamme, Davy Preuveneers, and Wouter Joosen (imec-DistriNet, KU Leuven)","Face authentication lacks key metrics to assess the robustness of users' representation within the system.
We fill the gap by investigating face uniqueness, which is the distinctiveness of a face within a population, as a proxy for robustness against adversarial examples.
By generating malicious input that escapes face verification, a dodging attack, we show a correlation between the amount of perturbation needed for successfully attacking a user and the uniqueness of the same user within a dataset.
Our experiments span over multiple networks under a realistic threat model, indicating that unique users are more resilient to gradient-based attacks than non-unique ones.",0,Accepted
22,Systematic Elicitation of Common Security Design Flaws,"Stef Verreydt, Laurens Sion, Koen Yskout, and Wouter Joosen (imec-Distrinet, KU Leuven)","Threat modeling allows potential security threats to be identified and mitigated at design time.
Countermeasures in current threat modeling approaches are mostly modeled as a boolean: either they are implemented, or they are not.
This does not allow to take into account potential design flaws for the countermeasure itself.
A considerable amount of security issues is, however, related to the wrong or incomplete application of common security tactics.
For example, the effectiveness of audit logs drops if the data written to the logs is not sanitized.
In this paper, we describe our novel approach which aims to systematically and automatically identify common security design flaws.",0,Accepted
23,The impact of data sampling in the anonymization pipeline,"Jenno Verdonck, Kevin De Boeck, Michiel Willocx, Jorn Lapon, and Vincent Naessens (KU Leuven - DistriNet)","An increasing number of companies are selling data as an additional source of revenue, or acquire data from other parties to optimize their business. In many cases, the shared data contains sensitive personal records. According to the GDPR regulation, personal data should be anonymized before it is released to third parties. A frequently applied technique is the k-anonymity metric, which ensures that every record in the dataset becomes indistinguishable from K other records through data generalization. This work combines generalization techniques with sampling. By adding a sampling step in the anonymization pipeline, additional uncertainty is introduced towards a potential attacker. As attackers can no longer be sure that an individual is in the sampled dataset, the re-identification risk is mitigated. This work proposes and evaluates multiple sampling techniques. Both the privacy and the utility properties of the anonymized datasets are embraced. The utility of the anonymized datasets is further evaluated in a machine learning use-case.",0,Accepted
24,The impact of public data during de-anonymization: a case study,"Kevin De Boeck, Jenno Verdonck, Michiel Willocx, Jorn Lapon, and Vincent Naessens (KU Leuven - DistriNet)","Many companies, non-profit organizations and governmental bodies collect personal information during service interactions. However, releasing sensitive personal data may impose huge privacy risks. First, an increasing amount of sensitive personal information becomes publicly available online after user consent.  Moreover, data breaches may result in huge data dumps that can contain personal records of millions of individuals.  Hence, malicious entities are able to scrape, collect and combine personal data from multiple sources in order to compile detailed profiles of many individuals. This paper demonstrates the impact of publicly available data during de-anonymization by means of a concrete case study. Journalists are often reluctant or even prohibited to release the identity of suspects or victims in criminal cases. They do, however, often release initials and background (such as their age and residential location). Through a large scale study of over 132.000 news articles, this paper demonstrates that currently applied privacy measures are often insufficient and straightforward re-identification strategies can de-anonymize individuals.",0,Accepted
27,Pillars of Sand: The current state of Datasets in the field of Network Intrusion Detection,"Gints Engelen (imec-DistriNet, KU Leuven); Robert Flood (University of Edinburgh); Lisa Liu-Thorrold (UNSW Canberra); Vera Rimmer (imec-DistriNet, KU Leuven); Henry Clausen and David Aspinall (University of Edinburgh); Wouter Joosen (imec-DistriNet, KU Leuven)","Network Intrusion Detection Systems play a critical role in protecting network architectures from harm. In the past decade, Machine Learning has moved to the forefront of research in this field, with many approaches resulting in great performance on benchmark NIDS datasets. The relevance of these performance results is however directly tied to the quality of the benchmark datasets used for training, which have so far not been subjected to thorough analysis. As part of our work, we have performed a large-scale manual investigation of the most commonly used publicly available NIDS datasets, where we have uncovered numerous errors due to problems in data pre-processing, attack simulation and labelling. We also highlight the lack of variability in both benign and malicious traffic, which often renders the classification task trivial. To quantify this variability, we have devised an automated methodology that can be applied without requiring expert domain knowledge. Nevertheless, we believe it is vital for any NIDS benchmark datasets to undergo a thorough manual analysis before being widely adopted. As a follow-up of our previous work where we provided an improved version of the CICIDS 2017 dataset, we are also actively working on improving the CSE-CIC-IDS 2018 dataset, which we intend to release to the research community.",0,Accepted